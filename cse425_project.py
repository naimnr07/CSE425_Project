# -*- coding: utf-8 -*-
"""CSE425_Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GV1FJGPZ12571tz_dxLtJQlcD21GEaR1
"""

# IMPORTS
import os
import json
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, TensorDataset
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.decomposition import PCA
from sklearn.metrics import (
    silhouette_score, calinski_harabasz_score, davies_bouldin_score,
    adjusted_rand_score, normalized_mutual_info_score
)
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
try:
    import umap
    UMAP_AVAILABLE = True
except ImportError:
    UMAP_AVAILABLE = False
    print("UMAP not available. Install with: pip install umap-learn")

try:
    import librosa
    LIBROSA_AVAILABLE = True
except ImportError:
    LIBROSA_AVAILABLE = False
    print("Librosa not available. Audio features will be disabled.")

try:
    import kagglehub
    KAGGLEHUB_AVAILABLE = True
except ImportError:
    KAGGLEHUB_AVAILABLE = False
    print("Kagglehub not available. Install with: pip install kagglehub")


# ============================================================================
# DATASET LOADING
# ============================================================================
class LyricsDataset(Dataset):
    """Dataset class for lyrics data."""

    def __init__(self, data_path, max_features=5000, normalize=True):
        self.data_path = data_path
        self.max_features = max_features
        self.normalize = normalize
        self.scaler = StandardScaler() if normalize else None

        self.data, self.labels = self._load_data()
        self.features = self._extract_features()

        if normalize:
            self.features = self.scaler.fit_transform(self.features)

    def _load_data(self):
        """Load lyrics dataset from CSV files."""
        data_files = []
        labels = []

        if os.path.isdir(self.data_path):
            for root, dirs, files in os.walk(self.data_path):
                for file in files:
                    if file.endswith('.csv'):
                        file_path = os.path.join(root, file)
                        try:
                            df = pd.read_csv(file_path)
                            text_col = None
                            lang_col = None

                            for col in df.columns:
                                col_lower = col.lower()
                                if 'lyric' in col_lower or 'text' in col_lower or 'song' in col_lower:
                                    text_col = col
                                if 'language' in col_lower or 'lang' in col_lower:
                                    lang_col = col

                            if text_col:
                                data_files.extend(df[text_col].astype(str).tolist())
                                if lang_col:
                                    labels.extend(df[lang_col].tolist())
                                else:
                                    labels.extend([None] * len(df))
                        except Exception as e:
                            print(f"Error loading {file_path}: {e}")

        return data_files, labels

    def _extract_features(self):
        """Extract TF-IDF features from lyrics."""
        if not self.data:
            raise ValueError("No data loaded. Check dataset path.")

        print(f"Extracting TF-IDF features from {len(self.data)} lyrics...")
        vectorizer = TfidfVectorizer(
            max_features=self.max_features,
            stop_words='english',
            ngram_range=(1, 2),
            min_df=2,
            max_df=0.95
        )

        features = vectorizer.fit_transform(self.data).toarray()
        self.vectorizer = vectorizer

        print(f"Extracted {features.shape[1]} features from {features.shape[0]} samples")
        return features

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        feature = torch.FloatTensor(self.features[idx])
        label = self.labels[idx] if self.labels[idx] is not None else -1
        return feature, label


def get_dataloader(data_path, batch_size=32, shuffle=True, max_features=5000):
    """Create DataLoader for lyrics dataset."""
    dataset = LyricsDataset(data_path, max_features=max_features)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=0)
    return dataloader, dataset


# ============================================================================
# VAE MODELS
# ============================================================================
class VAE(nn.Module):
    """Variational Autoencoder for learning latent representations."""

    def __init__(self, input_dim, latent_dim=32, hidden_dims=[512, 256]):
        super(VAE, self).__init__()
        self.input_dim = input_dim
        self.latent_dim = latent_dim

        # Encoder
        encoder_layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            encoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            encoder_layers.append(nn.ReLU())
            encoder_layers.append(nn.Dropout(0.2))
            prev_dim = hidden_dim

        self.encoder = nn.Sequential(*encoder_layers)
        self.fc_mu = nn.Linear(prev_dim, latent_dim)
        self.fc_logvar = nn.Linear(prev_dim, latent_dim)

        # Decoder
        decoder_layers = []
        prev_dim = latent_dim
        for hidden_dim in reversed(hidden_dims):
            decoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            decoder_layers.append(nn.ReLU())
            decoder_layers.append(nn.Dropout(0.2))
            prev_dim = hidden_dim

        decoder_layers.append(nn.Linear(prev_dim, input_dim))
        decoder_layers.append(nn.Sigmoid())
        self.decoder = nn.Sequential(*decoder_layers)

    def encode(self, x):
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        return self.decoder(z)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon_x = self.decode(z)
        return recon_x, mu, logvar, z


class BetaVAE(nn.Module):
    """Beta-VAE for learning disentangled representations."""

    def __init__(self, input_dim, latent_dim=32, beta=4.0, hidden_dims=[512, 256]):
        super(BetaVAE, self).__init__()
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        self.beta = beta

        encoder_layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            encoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            encoder_layers.append(nn.ReLU())
            encoder_layers.append(nn.Dropout(0.2))
            prev_dim = hidden_dim

        self.encoder = nn.Sequential(*encoder_layers)
        self.fc_mu = nn.Linear(prev_dim, latent_dim)
        self.fc_logvar = nn.Linear(prev_dim, latent_dim)

        decoder_layers = []
        prev_dim = latent_dim
        for hidden_dim in reversed(hidden_dims):
            decoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            decoder_layers.append(nn.ReLU())
            decoder_layers.append(nn.Dropout(0.2))
            prev_dim = hidden_dim

        decoder_layers.append(nn.Linear(prev_dim, input_dim))
        decoder_layers.append(nn.Sigmoid())
        self.decoder = nn.Sequential(*decoder_layers)

    def encode(self, x):
        h = self.encoder(x)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        return self.decoder(z)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon_x = self.decode(z)
        return recon_x, mu, logvar, z


class ConditionalVAE(nn.Module):
    """Conditional VAE (CVAE) that conditions on class labels."""

    def __init__(self, input_dim, latent_dim=32, condition_dim=10, hidden_dims=[512, 256]):
        super(ConditionalVAE, self).__init__()
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        self.condition_dim = condition_dim

        encoder_layers = []
        prev_dim = input_dim + condition_dim
        for hidden_dim in hidden_dims:
            encoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            encoder_layers.append(nn.ReLU())
            encoder_layers.append(nn.Dropout(0.2))
            prev_dim = hidden_dim

        self.encoder = nn.Sequential(*encoder_layers)
        self.fc_mu = nn.Linear(prev_dim, latent_dim)
        self.fc_logvar = nn.Linear(prev_dim, latent_dim)

        decoder_layers = []
        prev_dim = latent_dim + condition_dim
        for hidden_dim in reversed(hidden_dims):
            decoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            decoder_layers.append(nn.ReLU())
            decoder_layers.append(nn.Dropout(0.2))
            prev_dim = hidden_dim

        decoder_layers.append(nn.Linear(prev_dim, input_dim))
        decoder_layers.append(nn.Sigmoid())
        self.decoder = nn.Sequential(*decoder_layers)

    def encode(self, x, condition):
        if condition.dim() == 1:
            condition = F.one_hot(condition.long(), self.condition_dim).float()
        x_cond = torch.cat([x, condition], dim=1)
        h = self.encoder(x_cond)
        mu = self.fc_mu(h)
        logvar = self.fc_logvar(h)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z, condition):
        if condition.dim() == 1:
            condition = F.one_hot(condition.long(), self.condition_dim).float()
        z_cond = torch.cat([z, condition], dim=1)
        return self.decoder(z_cond)

    def forward(self, x, condition):
        mu, logvar = self.encode(x, condition)
        z = self.reparameterize(mu, logvar)
        recon_x = self.decode(z, condition)
        return recon_x, mu, logvar, z


class Autoencoder(nn.Module):
    """Standard Autoencoder (non-variational) for baseline comparison."""

    def __init__(self, input_dim, latent_dim=32, hidden_dims=[512, 256]):
        super(Autoencoder, self).__init__()
        self.input_dim = input_dim
        self.latent_dim = latent_dim

        encoder_layers = []
        prev_dim = input_dim
        for hidden_dim in hidden_dims:
            encoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            encoder_layers.append(nn.ReLU())
            encoder_layers.append(nn.Dropout(0.2))
            prev_dim = hidden_dim

        encoder_layers.append(nn.Linear(prev_dim, latent_dim))
        self.encoder = nn.Sequential(*encoder_layers)

        decoder_layers = []
        prev_dim = latent_dim
        for hidden_dim in reversed(hidden_dims):
            decoder_layers.append(nn.Linear(prev_dim, hidden_dim))
            decoder_layers.append(nn.ReLU())
            decoder_layers.append(nn.Dropout(0.2))
            prev_dim = hidden_dim

        decoder_layers.append(nn.Linear(prev_dim, input_dim))
        decoder_layers.append(nn.Sigmoid())
        self.decoder = nn.Sequential(*decoder_layers)

    def encode(self, x):
        return self.encoder(x)

    def decode(self, z):
        return self.decoder(z)

    def forward(self, x):
        z = self.encode(x)
        recon_x = self.decode(z)
        return recon_x, z


# ============================================================================
# TRAINING FUNCTIONS
# ============================================================================
def train_vae_model(model, dataloader, epochs=50, lr=1e-3, device='cpu', beta=1.0, verbose=True):
    """Train VAE model."""
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    history = {'total_loss': [], 'recon_loss': [], 'kl_loss': []}

    model.train()
    for epoch in range(epochs):
        total_loss_epoch = 0
        recon_loss_epoch = 0
        kl_loss_epoch = 0
        n_batches = 0

        for data, _ in dataloader:
            data = data.to(device)
            optimizer.zero_grad()

            recon_batch, mu, logvar, _ = model(data)

            recon_loss = F.mse_loss(recon_batch, data, reduction='sum')
            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

            loss = recon_loss + beta * kl_loss
            loss.backward()
            optimizer.step()

            total_loss_epoch += loss.item()
            recon_loss_epoch += recon_loss.item()
            kl_loss_epoch += kl_loss.item()
            n_batches += 1

        history['total_loss'].append(total_loss_epoch / n_batches)
        history['recon_loss'].append(recon_loss_epoch / n_batches)
        history['kl_loss'].append(kl_loss_epoch / n_batches)

        if verbose and (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Total Loss: {history["total_loss"][-1]:.4f}, '
                  f'Recon: {history["recon_loss"][-1]:.4f}, KL: {history["kl_loss"][-1]:.4f}')

    return history


def train_cvae_model(model, dataloader, epochs=50, lr=1e-3, device='cpu', verbose=True):
    """Train Conditional VAE model."""
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    history = {'total_loss': [], 'recon_loss': [], 'kl_loss': []}

    model.train()
    for epoch in range(epochs):
        total_loss_epoch = 0
        recon_loss_epoch = 0
        kl_loss_epoch = 0
        n_batches = 0

        for data, condition in dataloader:
            data = data.to(device)
            condition = condition.to(device)
            optimizer.zero_grad()

            recon_batch, mu, logvar, _ = model(data, condition)

            recon_loss = F.mse_loss(recon_batch, data, reduction='sum')
            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())

            loss = recon_loss + kl_loss
            loss.backward()
            optimizer.step()

            total_loss_epoch += loss.item()
            recon_loss_epoch += recon_loss.item()
            kl_loss_epoch += kl_loss.item()
            n_batches += 1

        history['total_loss'].append(total_loss_epoch / n_batches)
        history['recon_loss'].append(recon_loss_epoch / n_batches)
        history['kl_loss'].append(kl_loss_epoch / n_batches)

        if verbose and (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Total Loss: {history["total_loss"][-1]:.4f}')

    return history


def train_autoencoder(model, dataloader, epochs=50, lr=1e-3, device='cpu', verbose=True):
    """Train Autoencoder model."""
    model = model.to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=lr)

    history = {'loss': []}

    model.train()
    for epoch in range(epochs):
        loss_epoch = 0
        n_batches = 0

        for data, _ in dataloader:
            data = data.to(device)
            optimizer.zero_grad()

            recon_batch, _ = model(data)
            loss = F.mse_loss(recon_batch, data, reduction='sum')
            loss.backward()
            optimizer.step()

            loss_epoch += loss.item()
            n_batches += 1

        history['loss'].append(loss_epoch / n_batches)

        if verbose and (epoch + 1) % 10 == 0:
            print(f'Epoch [{epoch+1}/{epochs}], Loss: {history["loss"][-1]:.4f}')

    return history


# ============================================================================
# CLUSTERING
# ============================================================================
def kmeans_clustering(X, n_clusters=5, random_state=42):
    """Perform K-Means clustering."""
    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state, n_init=10)
    labels = kmeans.fit_predict(X)
    return labels


def agglomerative_clustering(X, n_clusters=5, linkage='ward'):
    """Perform Agglomerative Clustering."""
    clustering = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)
    labels = clustering.fit_predict(X)
    return labels


def dbscan_clustering(X, eps=0.5, min_samples=5):
    """Perform DBSCAN clustering."""
    clustering = DBSCAN(eps=eps, min_samples=min_samples)
    labels = clustering.fit_predict(X)
    return labels


def pca_baseline(X, n_components=32):
    """PCA baseline for dimensionality reduction."""
    pca = PCA(n_components=n_components)
    X_pca = pca.fit_transform(X)
    return X_pca, pca


# ============================================================================
# EVALUATION METRICS
# ============================================================================
def cluster_purity(true_labels, pred_labels):
    """Calculate Cluster Purity."""
    n = len(true_labels)
    clusters = np.unique(pred_labels)
    classes = np.unique(true_labels)

    purity_sum = 0
    for cluster in clusters:
        cluster_mask = pred_labels == cluster
        cluster_true_labels = true_labels[cluster_mask]

        if len(cluster_true_labels) == 0:
            continue

        max_intersection = 0
        for class_label in classes:
            intersection = np.sum(cluster_true_labels == class_label)
            max_intersection = max(max_intersection, intersection)

        purity_sum += max_intersection

    return purity_sum / n


def evaluate_clustering(X, pred_labels, true_labels=None):
    """Comprehensive clustering evaluation."""
    metrics = {}

    metrics['silhouette_score'] = silhouette_score(X, pred_labels) if len(np.unique(pred_labels)) >= 2 else -1.0
    metrics['calinski_harabasz'] = calinski_harabasz_score(X, pred_labels) if len(np.unique(pred_labels)) >= 2 else 0.0
    metrics['davies_bouldin'] = davies_bouldin_score(X, pred_labels) if len(np.unique(pred_labels)) >= 2 else float('inf')

    if true_labels is not None:
        mask = true_labels != -1
        if np.sum(mask) > 0:
            metrics['ari'] = adjusted_rand_score(true_labels[mask], pred_labels[mask])
            metrics['nmi'] = normalized_mutual_info_score(true_labels[mask], pred_labels[mask], average_method='arithmetic')
            metrics['purity'] = cluster_purity(true_labels[mask], pred_labels[mask])

    return metrics


def print_metrics(metrics, title="Clustering Metrics"):
    """Print metrics in a formatted way."""
    print(f"\n{title}")
    print("-" * 50)
    for metric_name, value in metrics.items():
        if isinstance(value, float):
            if np.isinf(value):
                print(f"{metric_name:25s}: inf")
            else:
                print(f"{metric_name:25s}: {value:.4f}")
        else:
            print(f"{metric_name:25s}: {value}")
    print("-" * 50)


# ============================================================================
# VISUALIZATION
# ============================================================================
def plot_tsne(X, labels, title="t-SNE Visualization", save_path=None):
    """Plot t-SNE visualization of data."""
    print("Computing t-SNE embedding...")
    tsne = TSNE(n_components=2, random_state=42, perplexity=30)
    X_tsne = tsne.fit_transform(X)

    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', alpha=0.6)
    plt.colorbar(scatter)
    plt.title(title)
    plt.xlabel("t-SNE 1")
    plt.ylabel("t-SNE 2")

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Saved t-SNE plot to {save_path}")

    plt.show()


def plot_umap(X, labels, title="UMAP Visualization", save_path=None):
    """Plot UMAP visualization of data."""
    if not UMAP_AVAILABLE:
        print("UMAP not available. Skipping UMAP visualization.")
        return

    print("Computing UMAP embedding...")
    reducer = umap.UMAP(n_components=2, random_state=42)
    X_umap = reducer.fit_transform(X)

    plt.figure(figsize=(10, 8))
    scatter = plt.scatter(X_umap[:, 0], X_umap[:, 1], c=labels, cmap='tab10', alpha=0.6)
    plt.colorbar(scatter)
    plt.title(title)
    plt.xlabel("UMAP 1")
    plt.ylabel("UMAP 2")

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Saved UMAP plot to {save_path}")

    plt.show()


def plot_training_history(history, save_path=None):
    """Plot training history."""
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))

    epochs = range(1, len(history['total_loss']) + 1)

    axes[0].plot(epochs, history['total_loss'], 'b-', label='Total Loss')
    axes[0].set_title('Total Loss')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].grid(True)

    axes[1].plot(epochs, history['recon_loss'], 'r-', label='Reconstruction Loss')
    axes[1].set_title('Reconstruction Loss')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Loss')
    axes[1].legend()
    axes[1].grid(True)

    axes[2].plot(epochs, history['kl_loss'], 'g-', label='KL Loss')
    axes[2].set_title('KL Divergence Loss')
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('Loss')
    axes[2].legend()
    axes[2].grid(True)

    plt.tight_layout()

    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"Saved training history to {save_path}")

    plt.show()


# ============================================================================
# DATASET DOWNLOAD
# ============================================================================
def download_dataset():
    """Download the song lyrics dataset from Kaggle."""
    if not KAGGLEHUB_AVAILABLE:
        print("Kagglehub not available. Please install: pip install kagglehub")
        return None

    print("Downloading dataset from Kaggle...")
    try:
        path = kagglehub.dataset_download("deepshah16/song-lyrics-dataset")
        print(f"Dataset downloaded to: {path}")
        return path
    except Exception as e:
        print(f"Error downloading dataset: {e}")
        return None


# ============================================================================
# EASY TASK
# ============================================================================
def run_easy_task(data_path, epochs=50, batch_size=32, latent_dim=32, n_clusters=5, max_features=5000):
    """Run Easy Task: Basic VAE + K-Means Clustering."""
    print("=" * 60)
    print("EASY TASK: Basic VAE + K-Means Clustering")
    print("=" * 60)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")

    # Load dataset
    print("\n[1/6] Loading dataset...")
    train_loader, dataset = get_dataloader(data_path, batch_size=batch_size, max_features=max_features)
    input_dim = dataset.features.shape[1]
    print(f"Dataset loaded: {len(dataset)} samples, {input_dim} features")

    # Initialize VAE
    print(f"\n[2/6] Initializing VAE...")
    model = VAE(input_dim=input_dim, latent_dim=latent_dim, hidden_dims=[512, 256])
    print(f"VAE model created: {sum(p.numel() for p in model.parameters())} parameters")

    # Train VAE
    print(f"\n[3/6] Training VAE for {epochs} epochs...")
    history = train_vae_model(model, train_loader, epochs=epochs, lr=1e-3, device=device)

    # Extract latent features
    print(f"\n[4/6] Extracting latent features...")
    model.eval()
    latent_features = []
    labels = []
    with torch.no_grad():
        for data, label in train_loader:
            data = data.to(device)
            _, mu, _, z = model(data)
            latent_features.append(z.cpu().numpy())
            labels.extend(label.numpy())
    latent_features = np.vstack(latent_features)
    labels = np.array(labels)
    print(f"Extracted latent features: {latent_features.shape}")

    # K-Means clustering
    print(f"\n[5/6] Performing K-Means clustering (n_clusters={n_clusters})...")
    vae_kmeans_labels = kmeans_clustering(latent_features, n_clusters=n_clusters)

    # Evaluate
    print("\nEvaluating VAE + K-Means clustering...")
    vae_metrics = evaluate_clustering(latent_features, vae_kmeans_labels,
                                      true_labels=labels if labels[0] != -1 else None)
    print_metrics(vae_metrics, "VAE + K-Means Metrics")

    # PCA baseline
    print(f"\n[6/6] Baseline: PCA + K-Means...")
    original_features = dataset.features
    X_pca, _ = pca_baseline(original_features, n_components=latent_dim)
    pca_kmeans_labels = kmeans_clustering(X_pca, n_clusters=n_clusters)

    pca_metrics = evaluate_clustering(X_pca, pca_kmeans_labels,
                                      true_labels=labels if labels[0] != -1 else None)
    print_metrics(pca_metrics, "PCA + K-Means Metrics")

    # Comparison
    print("\n" + "=" * 60)
    print("COMPARISON: VAE vs PCA Baseline")
    print("=" * 60)
    comparison_df = pd.DataFrame({
        'VAE + K-Means': [vae_metrics.get('silhouette_score', 0), vae_metrics.get('calinski_harabasz', 0)],
        'PCA + K-Means': [pca_metrics.get('silhouette_score', 0), pca_metrics.get('calinski_harabasz', 0)]
    }, index=['Silhouette Score', 'Calinski-Harabasz Index'])
    print(comparison_df)

    # Visualizations
    print("\nCreating visualizations...")
    plot_training_history(history)
    plot_tsne(latent_features, vae_kmeans_labels, title="VAE Latent Features - t-SNE")
    plot_umap(latent_features, vae_kmeans_labels, title="VAE Latent Features - UMAP")

    print("\n" + "=" * 60)
    print("Easy Task Completed!")
    print("=" * 60)

    return model, vae_metrics, pca_metrics


# ============================================================================
# MEDIUM TASK
# ============================================================================
def run_medium_task(data_path, epochs=50, batch_size=32, latent_dim=32, n_clusters=5, max_features=5000):
    """Run Medium Task: Enhanced VAE + Multiple Clustering Algorithms."""
    print("=" * 60)
    print("MEDIUM TASK: Enhanced VAE + Multiple Clustering Algorithms")
    print("=" * 60)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")

    # Load dataset
    print("\n[1/7] Loading dataset...")
    train_loader, dataset = get_dataloader(data_path, batch_size=batch_size, max_features=max_features)
    input_dim = dataset.features.shape[1]
    print(f"Dataset loaded: {len(dataset)} samples, {input_dim} features")

    labels = np.array([dataset.labels[i] if dataset.labels[i] is not None else -1
                      for i in range(len(dataset))])

    # Initialize enhanced VAE
    print(f"\n[2/7] Initializing enhanced VAE...")
    model = VAE(input_dim=input_dim, latent_dim=latent_dim, hidden_dims=[512, 256, 128])

    # Train VAE
    print(f"\n[3/7] Training VAE for {epochs} epochs...")
    history = train_vae_model(model, train_loader, epochs=epochs, lr=1e-3, device=device)

    # Extract latent features
    print(f"\n[4/7] Extracting latent features...")
    model.eval()
    latent_features = []
    with torch.no_grad():
        for data, _ in train_loader:
            data = data.to(device)
            _, mu, _, z = model(data)
            latent_features.append(z.cpu().numpy())
    latent_features = np.vstack(latent_features)
    print(f"Extracted latent features: {latent_features.shape}")

    # Multiple clustering algorithms
    print(f"\n[5/7] Performing clustering with multiple algorithms...")
    results = {}

    # K-Means
    print("  - K-Means...")
    kmeans_labels = kmeans_clustering(latent_features, n_clusters=n_clusters)
    results['kmeans'] = {'labels': kmeans_labels, 'n_clusters': len(np.unique(kmeans_labels))}

    # Agglomerative
    print("  - Agglomerative Clustering...")
    try:
        agg_labels = agglomerative_clustering(latent_features, n_clusters=n_clusters)
        results['agglomerative'] = {'labels': agg_labels, 'n_clusters': len(np.unique(agg_labels))}
    except Exception as e:
        print(f"  Agglomerative clustering failed: {e}")

    # DBSCAN
    print("  - DBSCAN...")
    dbscan_labels = dbscan_clustering(latent_features)
    results['dbscan'] = {'labels': dbscan_labels, 'n_clusters': len(np.unique(dbscan_labels[dbscan_labels != -1]))}

    # Evaluate all methods
    print(f"\n[6/7] Evaluating clustering methods...")
    all_metrics = {}
    for method_name, method_results in results.items():
        metrics = evaluate_clustering(latent_features, method_results['labels'],
                                     true_labels=labels if labels[0] != -1 else None)
        all_metrics[method_name] = metrics
        print_metrics(metrics, f"{method_name.upper()} Metrics")

    # Visualizations
    print(f"\n[7/7] Creating visualizations...")
    plot_training_history(history)
    plot_tsne(latent_features, kmeans_labels, title="K-Means Clustering - t-SNE")
    plot_umap(latent_features, kmeans_labels, title="K-Means Clustering - UMAP")

    # Summary
    print("\n" + "=" * 60)
    print("CLUSTERING COMPARISON SUMMARY")
    print("=" * 60)
    results_df = pd.DataFrame(all_metrics).T
    print(results_df[['silhouette_score', 'calinski_harabasz', 'davies_bouldin']])

    print("\n" + "=" * 60)
    print("Medium Task Completed!")
    print("=" * 60)

    return model, all_metrics


# ============================================================================
# HARD TASK
# ============================================================================
def run_hard_task(data_path, epochs=50, batch_size=32, latent_dim=32, n_clusters=5, max_features=5000, beta=4.0):
    """Run Hard Task: CVAE/Beta-VAE + Comprehensive Evaluation."""
    print("=" * 60)
    print("HARD TASK: CVAE/Beta-VAE + Comprehensive Evaluation")
    print("=" * 60)

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"Using device: {device}")

    # Load dataset
    print("\n[1/9] Loading dataset...")
    train_loader, dataset = get_dataloader(data_path, batch_size=batch_size, max_features=max_features)
    input_dim = dataset.features.shape[1]
    print(f"Dataset loaded: {len(dataset)} samples, {input_dim} features")

    labels = np.array([dataset.labels[i] if dataset.labels[i] is not None else -1
                      for i in range(len(dataset))])
    unique_labels = np.unique(labels[labels != -1])
    condition_dim = max(len(unique_labels), 5)

    # Prepare features tensor
    features_tensor = torch.FloatTensor(dataset.features)
    labels_tensor = torch.LongTensor(labels)
    fused_dataset = TensorDataset(features_tensor, labels_tensor)
    fused_loader = DataLoader(fused_dataset, batch_size=batch_size, shuffle=True)

    # Train Beta-VAE
    print(f"\n[2/9] Training Beta-VAE (beta={beta})...")
    beta_vae = BetaVAE(input_dim=input_dim, latent_dim=latent_dim, beta=beta, hidden_dims=[512, 256, 128])
    beta_history = train_vae_model(beta_vae, fused_loader, epochs=epochs, lr=1e-3, device=device, beta=beta)

    # Train CVAE (if labels available)
    cvae_model = None
    if len(unique_labels) > 0:
        print(f"\n[3/9] Training Conditional VAE (condition_dim={condition_dim})...")
        cvae = ConditionalVAE(input_dim=input_dim, latent_dim=latent_dim, condition_dim=condition_dim,
                             hidden_dims=[512, 256, 128])

        # Create condition dataloader
        label_encoder = LabelEncoder()
        encoded_labels = label_encoder.fit_transform(labels) % condition_dim
        condition_dataset = TensorDataset(features_tensor, torch.LongTensor(encoded_labels))
        condition_loader = DataLoader(condition_dataset, batch_size=batch_size, shuffle=True)

        cvae_history = train_cvae_model(cvae, condition_loader, epochs=epochs, lr=1e-3, device=device)
        cvae_model = cvae
    else:
        print("\n[3/9] Skipping CVAE (no labels available)")

    # Train Autoencoder
    print(f"\n[4/9] Training Autoencoder baseline...")
    autoencoder = Autoencoder(input_dim=input_dim, latent_dim=latent_dim, hidden_dims=[512, 256, 128])
    ae_history = train_autoencoder(autoencoder, fused_loader, epochs=epochs, lr=1e-3, device=device)

    # Extract latent features
    print(f"\n[5/9] Extracting latent features from all models...")

    # Beta-VAE
    beta_vae.eval()
    beta_latent = []
    with torch.no_grad():
        for data, _ in fused_loader:
            data = data.to(device)
            _, mu, _, z = beta_vae(data)
            beta_latent.append(z.cpu().numpy())
    beta_latent = np.vstack(beta_latent)

    # CVAE
    cvae_latent = None
    if cvae_model is not None:
        cvae_model.eval()
        cvae_latent = []
        with torch.no_grad():
            for data, condition in condition_loader:
                data = data.to(device)
                condition = condition.to(device)
                _, mu, _, z = cvae_model(data, condition)
                cvae_latent.append(z.cpu().numpy())
        cvae_latent = np.vstack(cvae_latent)

    # Autoencoder
    autoencoder.eval()
    ae_latent = []
    with torch.no_grad():
        for data, _ in fused_loader:
            data = data.to(device)
            _, z = autoencoder(data)
            ae_latent.append(z.cpu().numpy())
    ae_latent = np.vstack(ae_latent)

    # PCA baseline
    print(f"\n[6/9] Computing PCA baseline...")
    X_pca, _ = pca_baseline(dataset.features, n_components=latent_dim)

    # Clustering on all representations
    print(f"\n[7/9] Performing K-Means clustering on all representations...")
    clustering_results = {}

    methods = {
        'Beta-VAE': beta_latent,
        'Autoencoder': ae_latent,
        'PCA': X_pca
    }

    if cvae_latent is not None:
        methods['CVAE'] = cvae_latent

    for method_name, features in methods.items():
        labels_pred = kmeans_clustering(features, n_clusters=n_clusters)
        clustering_results[method_name] = {'features': features, 'labels': labels_pred}

    # Comprehensive evaluation
    print(f"\n[8/9] Comprehensive evaluation...")
    all_metrics = {}
    for method_name, results in clustering_results.items():
        metrics = evaluate_clustering(results['features'], results['labels'],
                                     true_labels=labels if labels[0] != -1 else None)
        all_metrics[method_name] = metrics
        print_metrics(metrics, f"{method_name} Metrics")

    # Visualizations
    print(f"\n[9/9] Creating visualizations...")
    plot_training_history(beta_history, save_path=None)

    best_features = clustering_results['Beta-VAE']['features']
    best_labels = clustering_results['Beta-VAE']['labels']
    plot_tsne(best_features, best_labels, title="Beta-VAE Latent Space - t-SNE")
    plot_umap(best_features, best_labels, title="Beta-VAE Latent Space - UMAP")

    # Summary
    print("\n" + "=" * 80)
    print("COMPREHENSIVE COMPARISON")
    print("=" * 80)
    results_df = pd.DataFrame(all_metrics).T
    comparison_cols = ['silhouette_score', 'calinski_harabasz', 'davies_bouldin']
    if labels[0] != -1:
        comparison_cols.extend(['ari', 'nmi', 'purity'])
    print(results_df[comparison_cols])

    print("\n" + "=" * 80)
    print("Hard Task Completed!")
    print("=" * 80)
    print(f"\nBest method by Silhouette Score: {results_df['silhouette_score'].idxmax()}")

    return beta_vae, all_metrics


# ============================================================================
# MAIN EXECUTION
# ============================================================================
if __name__ == "__main__":
    # Download dataset first
    print("=" * 60)
    print("VAE Music Clustering - Complete Project")
    print("=" * 60)

    # Download dataset
    dataset_path = download_dataset()

    if dataset_path is None:
        print("\nPlease download the dataset manually or update the path.")
        print("You can also set dataset_path directly in the code.")
        dataset_path = "data/lyrics"  # Default path

    print(f"\nUsing dataset path: {dataset_path}")

    # Configuration
    EPOCHS = 50
    BATCH_SIZE = 32
    LATENT_DIM = 32
    N_CLUSTERS = 5
    MAX_FEATURES = 5000

    # Run tasks (uncomment the ones you want to run)
    print("\n" + "=" * 60)
    print("Choose which task to run:")
    print("1. Easy Task")
    print("2. Medium Task")
    print("3. Hard Task")
    print("=" * 60)

    # Uncomment to run specific tasks:

    #Easy Task
    run_easy_task(dataset_path, epochs=EPOCHS, batch_size=BATCH_SIZE, latent_dim=LATENT_DIM, n_clusters=N_CLUSTERS, max_features=MAX_FEATURES)

    #Medium Task
    run_medium_task(dataset_path, epochs=EPOCHS, batch_size=BATCH_SIZE,latent_dim=LATENT_DIM, n_clusters=N_CLUSTERS, max_features=MAX_FEATURES)

    #Hard Task
    run_hard_task(dataset_path, epochs=EPOCHS, batch_size=BATCH_SIZE,latent_dim=LATENT_DIM, n_clusters=N_CLUSTERS, max_features=MAX_FEATURES, beta=4.0)

    print("\nUncomment the task you want to run in the code above!")